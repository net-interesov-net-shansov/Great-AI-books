# Решение проблем при обучении - улучшение общности
### Оценка моделей машинного обучения - способы оценки
- Проверка с простым расщеплением выборки
```python
num_validation_samples = 10000
# после расщепления желательно перемешать оставшиеся данные
np.random.shuffle(data)

# формирование проверочной выборки
validation_data = data[:num_validation_samples]
# формирование обучающей выборки
training_data = data[num_validation_samples:]

# обучение модели и проверка на проверочных данных
model = get_model()
model.fit(training_data, ...)
validation_score = model.evaluate(training_data, ...)

# далее возможна корректировка, повторное обучение, оценка и снова корректировка
...

# после настройки гиперпараметров желательно выполнить обучение окончательной модели на всех данных, не включённых в контрольный набор
model = get.model() ///
model.fit(np.concatenate([training_data, validating_data]), ...)
test_score = model.evaluate(test_data, ...)
```
        Самый простой протокол оценки, но он имеет существенный недостаток: При небольшом объёме данных проверочная и контрольная выборки могут содержать слишком мало образцов, чтобы  считаться статистически репрезентативными.
        Как это заметить - случайые перестановки данных перед расщеплением дают сильно отличающиеся оценки качества модели. В этом случае рекомендуется использовать другие методы оценки.

- 